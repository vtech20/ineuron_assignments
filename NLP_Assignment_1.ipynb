{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9499fdb4",
   "metadata": {},
   "source": [
    "1. Explain One-Hot Encoding\n",
    "2. Explain Bag of Words\n",
    "3. Explain Bag of N-Grams\n",
    "4. Explain TF-IDF\n",
    "5. What is OOV problem?\n",
    "6. What are word embeddings?\n",
    "7. Explain Continuous bag of words (CBOW)\n",
    "8. Explain SkipGram\n",
    "9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eaae05",
   "metadata": {},
   "source": [
    "**1. Explain One-Hot Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100469c",
   "metadata": {},
   "source": [
    "One hot encoding is a technique which converts categorical variables to numerical in an interpretable format. \n",
    "\n",
    "The advantages of using one hot encoding include:\n",
    "\n",
    "- It allows the use of categorical variables in models that require numerical input.\n",
    "- It can improve model performance by providing more information to the model about the categorical variable.\n",
    "- It can help to avoid the problem of ordinality, which can occur when a categorical variable has a natural ordering (e.g. “small”, “medium”, “large”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d03fa",
   "metadata": {},
   "source": [
    "**2. Explain Bag of Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e8c26",
   "metadata": {},
   "source": [
    "The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.\n",
    "\n",
    "The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and text/document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a11b9",
   "metadata": {},
   "source": [
    "**3. Explain Bag of N-Grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b511f",
   "metadata": {},
   "source": [
    "A bag-of-n-grams model records the number of times that each n-gram appears in each document of a collection. An n-gram is a collection of n successive words. bagOfNgrams does not split text into words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad76ca",
   "metadata": {},
   "source": [
    "**4. Explain TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcb0ff",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus.\n",
    "\n",
    "Term Frequency = Count of a word in a document/ Number of words in the document\n",
    "\n",
    "Inverse Document Frequency = Count of a word occurance in all the documents / number of document that a word present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87822b8d",
   "metadata": {},
   "source": [
    "TF-IDF = Term Frequent * Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646f550",
   "metadata": {},
   "source": [
    "**5. What is OOV problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c262916",
   "metadata": {},
   "source": [
    "Out-of-Vocabulary (OOV) is a problem for Neural Machine Translation (NMT). OOV refers to words with a low occurrence in the training data, or to those that are absent from the training data. To alleviate this, word or phrase-based Data Augmentation (DA) techniques have been used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0962aec",
   "metadata": {},
   "source": [
    "**6. What are word embeddings?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb44227",
   "metadata": {},
   "source": [
    "A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
    "\n",
    "It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131eac28",
   "metadata": {},
   "source": [
    "**7. Explain Continuous bag of words (CBOW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dab3b2",
   "metadata": {},
   "source": [
    "Word2vec is considered one of the biggest breakthroughs in the development of natural language processing. The reason behind this is because it is easy to understand and use. Word2vec is basically a word embedding technique that is used to convert the words in the dataset to vectors so that the machine understands. Each unique word in your data is assigned to a vector and these vectors vary in dimensions depending on the length of the word. \n",
    "\n",
    "The word2vec model has two different architectures to create the word embeddings. They are:\n",
    "\n",
    "- Continuous bag of words(CBOW)\n",
    "- Skip-gram model \n",
    "\n",
    "The CBOW model tries to understand the context of the words and takes this as input. It then tries to predict words that are contextually accurate. Let us consider an example for understanding this. Consider the sentence: ‘It is a pleasant day’ and the word ‘pleasant’ goes as input to the neural network. We are trying to predict the word ‘day’ here. We will use the one-hot encoding for the input words and measure the error rates with the one-hot encoded target word. Doing this will help us predict the output based on the word with least error. \n",
    "\n",
    "The model tries to predict the target word by trying to understand the context of the surrounding words. Consider the same sentence as above, ‘It is a pleasant day’.The model converts this sentence into word pairs in the form (contextword, targetword). The user will have to set the window size. If the window for the context word is 2 then the word pairs would look like this: ([it, a], is), ([is, pleasant], a),([a, day], pleasant). With these word pairs, the model tries to predict the target word considered the context words. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27ac4a",
   "metadata": {},
   "source": [
    "**8. Explain SkipGram**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b7eb5",
   "metadata": {},
   "source": [
    "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It's reverse of CBOW algorithm. Here, target word is input while context words are output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ec29b",
   "metadata": {},
   "source": [
    "**9. Explain Glove Embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c88513",
   "metadata": {},
   "source": [
    "The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics.  Unlike the occurrence matrix, the co-occurrence matrix tells you how often a particular word pair occurs together. Each value in the co-occurrence matrix represents a pair of words occurring together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60097e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
